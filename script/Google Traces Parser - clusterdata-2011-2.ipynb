{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Google Traces Parser - clusterdata-2011-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python notebook parse the google trace called \"clusterdata-2011-2\" and create the necessary trace files required to run \"cluster-scheduler-simulator\".\n",
    "\n",
    "Usefull links:\n",
    "<ul>\n",
    "    <li>\n",
    "        <a href=\"https://github.com/google/cluster-data\"> GitHub </a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"https://drive.google.com/open?id=0B5g07T_gRDg9Z0lsSTEtTWtpOW8&authuser=0\"> Format + Schema Document </a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"https://groups.google.com/forum/#!forum/googleclusterdata-discuss\"> Mailing List </a>\n",
    "    </li>\n",
    "</ul>\n",
    "Notes:\n",
    "<ul>\n",
    "    <li>\n",
    "        All resources utilization are normalized as explained in the \"Format + Schema Document\" in the above link.\n",
    "        For this reason we will adjust them to real values using the same cell size of the \"cluster-scheduler-simulator\".\n",
    "        File \"init-cluster-state.log\" expect real value of CPU and Memory to calculate the distribution.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext \n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "import os\n",
    "import errno\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Config values\n",
    "CLUSTER_INFO = {\n",
    "    \"mem_machine\": 64 * (1024**3),\n",
    "    \"cpu_machine\": 32\n",
    "}\n",
    "\n",
    "SIMULATOR_TRACES_OUTPUT_FOLDER = \"google-simulator-traces/\"\n",
    "JOB_TRACES_OUTPUT_FOLDER = \"job-distribution-traces/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    path = path.replace(\" \", \"_\")\n",
    "    dir_path = os.path.dirname(path)\n",
    "    try:\n",
    "        os.makedirs(dir_path)\n",
    "    except OSError as exc:  # Python >2.5\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(dir_path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "    return path\n",
    "\n",
    "# Create output folder structure\n",
    "SIMULATOR_TRACES_OUTPUT_FOLDER = mkdir_p(os.path.join(\".\", SIMULATOR_TRACES_OUTPUT_FOLDER))\n",
    "JOB_TRACES_OUTPUT_FOLDER = mkdir_p(os.path.join(SIMULATOR_TRACES_OUTPUT_FOLDER, JOB_TRACES_OUTPUT_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+------------+-------------------------------+----------------------+---------+\n",
      "|file_pattern                                 |field_number|content                        |format                |mandatory|\n",
      "+---------------------------------------------+------------+-------------------------------+----------------------+---------+\n",
      "|job_events/part-?????-of-?????.csv.gz        |1           |time                           |INTEGER               |YES      |\n",
      "|job_events/part-?????-of-?????.csv.gz        |2           |missing info                   |INTEGER               |NO       |\n",
      "|job_events/part-?????-of-?????.csv.gz        |3           |job ID                         |INTEGER               |YES      |\n",
      "|job_events/part-?????-of-?????.csv.gz        |4           |event type                     |INTEGER               |YES      |\n",
      "|job_events/part-?????-of-?????.csv.gz        |5           |user                           |STRING_HASH           |NO       |\n",
      "|job_events/part-?????-of-?????.csv.gz        |6           |scheduling class               |INTEGER               |NO       |\n",
      "|job_events/part-?????-of-?????.csv.gz        |7           |job name                       |STRING_HASH           |NO       |\n",
      "|job_events/part-?????-of-?????.csv.gz        |8           |logical job name               |STRING_HASH           |NO       |\n",
      "|task_events/part-?????-of-?????.csv.gz       |1           |time                           |INTEGER               |YES      |\n",
      "|task_events/part-?????-of-?????.csv.gz       |2           |missing info                   |INTEGER               |NO       |\n",
      "|task_events/part-?????-of-?????.csv.gz       |3           |job ID                         |INTEGER               |YES      |\n",
      "|task_events/part-?????-of-?????.csv.gz       |4           |task index                     |INTEGER               |YES      |\n",
      "|task_events/part-?????-of-?????.csv.gz       |5           |machine ID                     |INTEGER               |NO       |\n",
      "|task_events/part-?????-of-?????.csv.gz       |6           |event type                     |INTEGER               |YES      |\n",
      "|task_events/part-?????-of-?????.csv.gz       |7           |user                           |STRING_HASH           |NO       |\n",
      "|task_events/part-?????-of-?????.csv.gz       |8           |scheduling class               |INTEGER               |NO       |\n",
      "|task_events/part-?????-of-?????.csv.gz       |9           |priority                       |INTEGER               |YES      |\n",
      "|task_events/part-?????-of-?????.csv.gz       |10          |CPU request                    |FLOAT                 |NO       |\n",
      "|task_events/part-?????-of-?????.csv.gz       |11          |memory request                 |FLOAT                 |NO       |\n",
      "|task_events/part-?????-of-?????.csv.gz       |12          |disk space request             |FLOAT                 |NO       |\n",
      "|task_events/part-?????-of-?????.csv.gz       |13          |different machines restriction |BOOLEAN               |NO       |\n",
      "|machine_events/part-00000-of-00001.csv.gz    |1           |time                           |INTEGER               |YES      |\n",
      "|machine_events/part-00000-of-00001.csv.gz    |2           |machine ID                     |INTEGER               |YES      |\n",
      "|machine_events/part-00000-of-00001.csv.gz    |3           |event type                     |INTEGER               |YES      |\n",
      "|machine_events/part-00000-of-00001.csv.gz    |4           |platform ID                    |STRING_HASH           |NO       |\n",
      "|machine_events/part-00000-of-00001.csv.gz    |5           |CPUs                           |FLOAT                 |NO       |\n",
      "|machine_events/part-00000-of-00001.csv.gz    |6           |Memory                         |FLOAT                 |NO       |\n",
      "|machine_attributes/part-00000-of-00001.csv.gz|1           |time                           |INTEGER               |YES      |\n",
      "|machine_attributes/part-00000-of-00001.csv.gz|2           |machine ID                     |INTEGER               |YES      |\n",
      "|machine_attributes/part-00000-of-00001.csv.gz|3           |attribute name                 |STRING_HASH           |YES      |\n",
      "|machine_attributes/part-00000-of-00001.csv.gz|4           |attribute value                |STRING_HASH_OR_INTEGER|NO       |\n",
      "|machine_attributes/part-00000-of-00001.csv.gz|5           |attribute deleted              |BOOLEAN               |YES      |\n",
      "|task_constraints/part-?????-of-?????.csv.gz  |1           |time                           |INTEGER               |YES      |\n",
      "|task_constraints/part-?????-of-?????.csv.gz  |2           |job ID                         |INTEGER               |YES      |\n",
      "|task_constraints/part-?????-of-?????.csv.gz  |3           |task index                     |INTEGER               |YES      |\n",
      "|task_constraints/part-?????-of-?????.csv.gz  |4           |comparison operator            |INTEGER               |YES      |\n",
      "|task_constraints/part-?????-of-?????.csv.gz  |5           |attribute name                 |STRING_HASH           |YES      |\n",
      "|task_constraints/part-?????-of-?????.csv.gz  |6           |attribute value                |STRING_HASH_OR_INTEGER|NO       |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |1           |start time                     |INTEGER               |YES      |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |2           |end time                       |INTEGER               |YES      |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |3           |job ID                         |INTEGER               |YES      |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |4           |task index                     |INTEGER               |YES      |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |5           |machine ID                     |INTEGER               |YES      |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |6           |CPU rate                       |FLOAT                 |NO       |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |7           |canonical memory usage         |FLOAT                 |NO       |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |8           |assigned memory usage          |FLOAT                 |NO       |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |9           |unmapped page cache            |FLOAT                 |NO       |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |10          |total page cache               |FLOAT                 |NO       |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |11          |maximum memory usage           |FLOAT                 |NO       |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |12          |disk I/O time                  |FLOAT                 |NO       |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |13          |local disk space usage         |FLOAT                 |NO       |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |14          |maximum CPU rate               |FLOAT                 |NO       |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |15          |maximum disk IO time           |FLOAT                 |NO       |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |16          |cycles per instruction         |FLOAT                 |NO       |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |17          |memory accesses per instruction|FLOAT                 |NO       |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |18          |sample portion                 |FLOAT                 |NO       |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |19          |aggregation type               |BOOLEAN               |NO       |\n",
      "|task_usage/part-?????-of-?????.csv.gz        |20          |sampled CPU usage              |FLOAT                 |NO       |\n",
      "+---------------------------------------------+------------+-------------------------------+----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# conf = (\n",
    "#     SparkConf()\n",
    "#     .set(\"spark.master\", os.environ[\"SPARK_MASTER\"])\n",
    "#     .set(\"spark.executor.memory\", os.environ[\"SPARK_EXECUTOR_RAM\"])\n",
    "#        )\n",
    "# sc.stop()\n",
    "# sc=SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"file_pattern\", StringType(), False), \\\n",
    "    StructField(\"field_number\", IntegerType(), False), \\\n",
    "    StructField(\"content\", StringType(), False), \\\n",
    "    StructField(\"format\", StringType(), False), \\\n",
    "    StructField(\"mandatory\", StringType(), False)])\n",
    "schema_all = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true').load(\"clusterdata-2011-2/schema.csv\", schema=schema)\n",
    "# Displays the content of the DataFrame to stdout\n",
    "schema_all.show(n=schema_all.count(), truncate=False)\n",
    "format_type = {\n",
    "    \"INTEGER\": DecimalType(32,0),\n",
    "    \"STRING_HASH\": StringType(),\n",
    "    \"FLOAT\": FloatType(),\n",
    "    \"BOOLEAN\": DecimalType(),\n",
    "    \"STRING_HASH_OR_INTEGER\": StringType()    \n",
    "}\n",
    "schemas = {}\n",
    "for line in schema_all.orderBy(\"file_pattern\", \"field_number\").select(\"*\").collect():\n",
    "    line = line.asDict()\n",
    "    try:\n",
    "        if line[\"file_pattern\"] not in schemas:\n",
    "            schemas[line[\"file_pattern\"]] = []\n",
    "\n",
    "        schemas[line[\"file_pattern\"]].append(\n",
    "            StructField(\n",
    "                line[\"content\"].replace(' ', '_'),\n",
    "                format_type[line[\"format\"]],\n",
    "                line[\"mandatory\"] != \"YES\"\n",
    "            )\n",
    "        )\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "\n",
    "dataframes = {}\n",
    "for file_pattern in schemas:\n",
    "    dataframes[os.path.dirname(file_pattern)] = sqlContext.read.format(\"com.databricks.spark.csv\").load(\"clusterdata-2011-2/\" + file_pattern, schema=StructType(schemas[file_pattern]))\n",
    "    \n",
    "max_time = pow(2,63) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "job_events = dataframes[\"job_events\"]\n",
    "#job_events.show(truncate=False)\n",
    "\n",
    "task_events = dataframes[\"task_events\"]\n",
    "#task_events.show(truncate=False)\n",
    "\n",
    "task_usage = dataframes[\"task_usage\"]\n",
    "#task_usage.show(truncate=False)\n",
    "\n",
    "machine_events = dataframes[\"machine_events\"]\n",
    "#machine_events.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs submitted before the event time window: 3984\n"
     ]
    }
   ],
   "source": [
    "jobs_already_running = job_events[(job_events.event_type == 1) & (job_events.time == 0)]\\\n",
    "                        .select(job_events.job_ID, job_events.scheduling_class)\n",
    "print(\"Jobs submitted before the event time window: {}\".format(jobs_already_running.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs submitted before that endend during the events time window: 79\n"
     ]
    }
   ],
   "source": [
    "finished_jobs_already_running = {}\n",
    "for job in job_events[job_events.event_type == 4].join(jobs_already_running, on=\"job_ID\")\\\n",
    "            .select(job_events.job_ID, job_events.time).collect():\n",
    "        if job.job_ID not in finished_jobs_already_running:\n",
    "            finished_jobs_already_running[job.job_ID] = job.time\n",
    "print(\"Jobs submitted before that endend during the events time window: {}\".format(len(finished_jobs_already_running)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production jobs submitted before the events time window: 2525\n"
     ]
    }
   ],
   "source": [
    "production_jobs_already_running = {}\n",
    "for job in task_events[(task_events.event_type == 0) & (task_events.priority >= 9)].join(jobs_already_running, on=\"job_ID\")\\\n",
    "            .groupBy(task_events.job_ID).count().collect():\n",
    "        if job.job_ID not in production_jobs_already_running:\n",
    "            production_jobs_already_running[job.job_ID] = 1\n",
    "print(\"Production jobs submitted before the events time window: {}\".format(len(production_jobs_already_running)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs submitted before the event time window that have at least 1 task: 3979\n"
     ]
    }
   ],
   "source": [
    "tasks_jobs_already_running = {}\n",
    "for job in task_events[task_events.event_type == 0].join(jobs_already_running, on=\"job_ID\")\\\n",
    "            .groupBy(task_events.job_ID).agg(func.max(\"task_index\").alias(\"tasks\")).collect():\n",
    "        if job.job_ID not in tasks_jobs_already_running:\n",
    "            tasks_jobs_already_running[job.job_ID] = job.tasks + 1 # task_index starts from 0\n",
    "print(\"Jobs submitted before the event time window that have at least 1 task: {}\".format(len(tasks_jobs_already_running)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs processed: 3976\n"
     ]
    }
   ],
   "source": [
    "resources_jobs_already_running = {}\n",
    "for job in task_usage.join(jobs_already_running, on=\"job_ID\")\\\n",
    "            .groupBy(task_usage.job_ID, task_usage.task_index)\\\n",
    "            .agg(\\\n",
    "                 func.avg(\"CPU_rate\").alias(\"cpu\"),\\\n",
    "                 func.avg(\"canonical_memory_usage\").alias(\"memory\")\n",
    "                )\\\n",
    "            .groupBy(task_usage.job_ID)\\\n",
    "            .agg(\\\n",
    "                 func.sum(\"cpu\").alias(\"cpu\"),\\\n",
    "                 func.sum(\"memory\").alias(\"memory\")\n",
    "                )\\\n",
    "            .collect():\n",
    "        if job.job_ID not in resources_jobs_already_running:\n",
    "            resources_jobs_already_running[job.job_ID] = {\n",
    "                'cpu': CLUSTER_INFO[\"cpu_machine\"] * job.cpu,\n",
    "                'memory': CLUSTER_INFO[\"mem_machine\"] * job.memory\n",
    "            }\n",
    "print(\"Jobs processed: {}\".format(len(resources_jobs_already_running)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs processed: 3979\n"
     ]
    }
   ],
   "source": [
    "job_count = 0\n",
    "with open(os.path.join(SIMULATOR_TRACES_OUTPUT_FOLDER, \"init-cluster-state.log\"), \"w\") as init_cluster_state:\n",
    "    for job in jobs_already_running.collect():\n",
    "        # === Common Columns ===\n",
    "        # Column 0: possible values are 11 or 12\n",
    "        #   \"11\" - (8 column schema) something that was there at the beginning of timewindow\n",
    "        #   \"12\" - (6 column schema) something that was there at beginning of timewindow and ended at [timestamp] (see Column 1)\n",
    "        # Column 1: timestamp\n",
    "        # Column 2: unique job ID\n",
    "        # Column 3: 0 or 1 - prod_job - boolean flag indicating if this job is \"production\" priority as described in [1]\n",
    "        # Column 4: 0, 1, 2, or 3 - sched_class - see description of \"Scheduling Class\" in [1]\n",
    "        column_0 = 11\n",
    "        column_1 = 0\n",
    "        column_2 = job.job_ID\n",
    "        column_3 = 0\n",
    "        column_4 = job.scheduling_class\n",
    "        # === 6 column format ===\n",
    "        # Column 5: UNSPECIFIED/UNUSED\n",
    "        #\n",
    "        # === 8 column format ===\n",
    "        # Column 5: number of tasks\n",
    "        # Column 6: aggregate CPU usage of job (in num cores)\n",
    "        # Column 7: aggregate Ram usage of job (in bytes)\n",
    "        column_5 = 0\n",
    "        column_6 = 0\n",
    "        column_7 = 0\n",
    "        \n",
    "        if job.job_ID in production_jobs_already_running:\n",
    "            column_3 = 1\n",
    "        \n",
    "        if job.job_ID in tasks_jobs_already_running:\n",
    "            column_5 = tasks_jobs_already_running[job.job_ID]\n",
    "        \n",
    "        # What's the point in adding a job with 0 tasks to the trace?\n",
    "        # Also, if we do so we will get an error\n",
    "        if column_5 == 0:\n",
    "            continue\n",
    "        \n",
    "        if job.job_ID in resources_jobs_already_running:\n",
    "            column_6 = resources_jobs_already_running[job.job_ID][\"cpu\"]\n",
    "            column_7 = int(resources_jobs_already_running[job.job_ID][\"memory\"])\n",
    "            \n",
    "        init_cluster_state.write(\"{} {} {} {} {} {} {} {}\\n\".format(\n",
    "                column_0, column_1, column_2, column_3, column_4, column_5, column_6, column_7))\n",
    "        \n",
    "        if job.job_ID in finished_jobs_already_running:\n",
    "            column_0 = 12\n",
    "            column_1 = finished_jobs_already_running[job.job_ID] \n",
    "            init_cluster_state.write(\"{} {} {} {} {} {}\\n\".format(\n",
    "                column_0, column_1, column_2, column_3, column_4, column_5))\n",
    "        job_count += 1\n",
    "print(\"Jobs processed: {}\".format(job_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_that_finished = job_events[((job_events.event_type == 1) | (job_events.event_type == 4))  & ((job_events.time > 0) & (job_events.time < max_time))]\\\n",
    "                    .orderBy(job_events.time).groupBy(job_events.job_ID).count().where(\"count = 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs submitted during the event time window: 385502\n"
     ]
    }
   ],
   "source": [
    "jobs = OrderedDict({})\n",
    "for job in job_events[(job_events.event_type == 1)].join(job_that_finished, on=\"job_ID\")\\\n",
    "                    .orderBy(job_events.time).select(job_events.job_ID, job_events.time, job_events.scheduling_class)\\\n",
    "                    .collect():\n",
    "        if job.job_ID not in jobs:\n",
    "            jobs[job.job_ID] = {\n",
    "                \"production\": False,\n",
    "                \"tasks\": 0,\n",
    "                \"start\": 0,\n",
    "                \"end\": 0,\n",
    "                \"scheduling_class\": None\n",
    "            }\n",
    "        jobs[job.job_ID][\"start\"] = job.time\n",
    "        jobs[job.job_ID][\"scheduling_class\"] = job.scheduling_class\n",
    "print(\"Jobs submitted during the event time window: {}\".format(len(jobs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs processed: 385502\n"
     ]
    }
   ],
   "source": [
    "job_count = 0\n",
    "for job in job_events[(job_events.event_type == 4)].join(job_that_finished, on=\"job_ID\")\\\n",
    "                .select(job_events.job_ID, job_events.time).collect():\n",
    "        if job.job_ID in jobs:\n",
    "            jobs[job.job_ID][\"end\"] = job.time\n",
    "            job_count += 1\n",
    "print(\"Jobs processed: {}\".format(job_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs processed: 385482\n"
     ]
    }
   ],
   "source": [
    "job_count = 0\n",
    "for job in task_events[task_events.event_type == 0].join(job_that_finished, on=\"job_ID\")\\\n",
    "                .groupBy(task_events.job_ID).agg(func.max(\"task_index\").alias(\"tasks\")).collect():\n",
    "        if job.job_ID in jobs:\n",
    "            jobs[job.job_ID][\"tasks\"] = job.tasks + 1\n",
    "            job_count += 1\n",
    "print(\"Jobs processed: {}\".format(job_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs processed: 35838\n"
     ]
    }
   ],
   "source": [
    "job_count = 0\n",
    "for job in task_events[(task_events.event_type == 0) & (task_events.priority >= 9)].join(job_that_finished, on=\"job_ID\") \\\n",
    "        .groupBy(task_events.job_ID).count().collect():\n",
    "    if job.job_ID in jobs:\n",
    "        jobs[job.job_ID][\"production\"] = True\n",
    "        job_count += 1\n",
    "print(\"Jobs processed: {}\".format(job_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interarrival_cmb = open(os.path.join(JOB_TRACES_OUTPUT_FOLDER, \"interarrival_cmb.log\"), \"w\")\n",
    "runtimes_cmb = open(os.path.join(JOB_TRACES_OUTPUT_FOLDER, \"runtimes_cmb.log\"), \"w\")\n",
    "csizes_cmb = open(os.path.join(JOB_TRACES_OUTPUT_FOLDER, \"csizes_cmb.log\"), \"w\")\n",
    "\n",
    "previous_arrival = 0\n",
    "for job in jobs:\n",
    "    job = jobs[job]\n",
    "    # What's the point in adding a job with 0 tasks to the trace?\n",
    "    # Also, if we do so we will get an error\n",
    "    if job[\"tasks\"] == 0:\n",
    "        continue\n",
    "        \n",
    "    # === Columns ===\n",
    "    # Column 0: cluster_name\n",
    "    # Column 1: assignment policy (\"cmb-new\" = \"CMB_PBB\")\n",
    "    # Column 2: scheduler id, values can be 0 or 1. 0 = batch, service = 1\n",
    "    # Column 3: depending on which trace file:\n",
    "    #     interarrival time (seconds since last job arrival)\n",
    "    #     OR tasks in job\n",
    "    #     OR job runtime (seconds)\n",
    "    column_0 = \"test\"\n",
    "    column_1 = \"cmb-new\"\n",
    "    column_2 = 1 if job[\"production\"] and (job[\"scheduling_class\"] != 0 and job[\"scheduling_class\"] != 1) else 0\n",
    "\n",
    "    if job[\"start\"] - previous_arrival < 0:\n",
    "        print(\"Error. Dataset is not ordered by arrival time!\")\n",
    "        break\n",
    "    interarrival_cmb.write(\"{} {} {} {} \\n\".format(column_0, column_1, column_2,\n",
    "                                                   (job[\"start\"] - previous_arrival)/1000000))\n",
    "    runtimes_cmb.write(\"{} {} {} {} \\n\".format(column_0, column_1, column_2,\n",
    "                                               (job[\"end\"] - job[\"start\"])/1000000))\n",
    "    csizes_cmb.write(\"{} {} {} {} \\n\".format(column_0, column_1, column_2,\n",
    "                                             job[\"tasks\"]))\n",
    "    \n",
    "    previous_arrival = job[\"start\"]\n",
    "\n",
    "interarrival_cmb.close()\n",
    "runtimes_cmb.close()\n",
    "csizes_cmb.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}